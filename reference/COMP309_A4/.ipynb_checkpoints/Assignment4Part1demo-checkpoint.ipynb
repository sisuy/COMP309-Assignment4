{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#from sklearn.neighbors import KNeighborsClassifier\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_loss\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gradient_descent, pso, mini_batch_gradient_descent\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utilities'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import *\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from pandas import Series\n",
    "\n",
    "from utilities.losses import compute_loss\n",
    "from utilities.optimizers import gradient_descent, pso, mini_batch_gradient_descent\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# General settings\n",
    "from utilities.visualization import visualize_train, visualize_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 309\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_test_split_test_size = 0.3\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.1  # step size\n",
    "max_iters = 50  # max iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from CSV\n",
    "# :return: df    a panda data frame\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"../data/Part 1 - regression/diamonds.csv\")\n",
    "    return df\n",
    "#load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data):\n",
    "    \"\"\"\n",
    "    Data preprocess:\n",
    "        1. Split the entire dataset into train and test\n",
    "        2. Split outputs and inputs\n",
    "        3. Standardize train and test\n",
    "        4. Add intercept dummy for computation convenience\n",
    "    :param data: the given dataset (format: panda DataFrame)\n",
    "    :return: train_data       train data contains only inputs\n",
    "             train_labels     train data contains only labels\n",
    "             test_data        test data contains only inputs\n",
    "             test_labels      test data contains only labels\n",
    "             train_data_full       train data (full) contains both inputs and labels\n",
    "             test_data_full       test data (full) contains both inputs and labels\n",
    "    \"\"\"\n",
    "    # drop the index attributes\n",
    "    data = data.drop(data.columns[0], axis=1)\n",
    "    \n",
    "    # replace cut column attribute 'Fair','Good','Very Good','Ideal','Premium' to 1 2 3 4 5\n",
    "    data=data.replace(['Fair','Good','Very Good','Ideal','Premium'],[1,2,3,4,5]);\n",
    "    # replace colour column attribute 'D','E','F','G','H','I','J' to 7,6,5,4,3,2,1\n",
    "    data=data.replace(['D','E','F','G','H','I','J'],[7,6,5,4,3,2,1]);\n",
    "    # replace clarity column attribute 'Fair','Good','Very Good','Ideal','Premium' to 1 2 3 4 5\n",
    "    data=data.replace(['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'],[1,2,3,4,5,6,7,8]);\n",
    "    \n",
    "    #print(data)\n",
    "    \n",
    "    # Split the data into train and test\n",
    "   \n",
    "    train_data, test_data = train_test_split(data, test_size = train_test_split_test_size)\n",
    "    \n",
    "    # Pre-process data (both train and test)\n",
    "    train_data_full = train_data.copy()\n",
    "    train_data = train_data.drop([\"price\"], axis = 1)\n",
    "    train_labels = train_data_full[\"price\"]\n",
    "\n",
    "    test_data_full = test_data.copy()\n",
    "    test_data = test_data.drop([\"price\"], axis = 1)\n",
    "    test_labels = test_data_full[\"price\"]\n",
    "\n",
    "    #Standardize the inputs\n",
    "    \n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "    \n",
    "    #print(train_data)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "[1662.52478923 1362.88105507 4160.22622514 ... 3345.71384124 3606.46924557\n",
      " 1073.31759633]\n",
      "Learn: execution time=78.684 seconds\n",
      "R2: 0.50\n",
      "MSE: 8164863.24\n",
      "RMSE: 2857.42\n",
      "MAE: 1356.06\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Settings\n",
    "    # Step 1: Load Data\n",
    "    data = load_data()\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(data)\n",
    "    \n",
    "    # Step 3: Learning Start\n",
    "    \n",
    "    start_time = datetime.datetime.now()  # Track learning starting time\n",
    "    print(train_labels.dtypes)\n",
    "    #Regression\n",
    "    baseline = LinearRegression()\n",
    "#     baseline = KNeighborsRegressor(n_neighbors=11)\n",
    "#     baseline = Ridge();\n",
    "#     baseline = DecisionTreeRegressor(max_depth=9)\n",
    "#     baseline = RandomForestRegressor(max_depth=9, random_state=0,n_estimators=500)\n",
    "#     baseline = GradientBoostingRegressor(n_estimators= 500, max_depth= 9, min_samples_split= 2,learning_rate= 0.15, loss= 'ls')\n",
    "#     baseline = SGDRegressor()\n",
    "#     baseline = SVR()\n",
    "#     baseline = LinearSVR()\n",
    "#     baseline = MLPRegressor(learning_rate_init=0.2)\n",
    "\n",
    "    baseline.fit(train_data,train_labels)\n",
    "    \n",
    "    #Prediction\n",
    "    y_pred = baseline.predict(test_data)\n",
    "    print(y_pred)\n",
    "    \n",
    "    end_time = datetime.datetime.now()  # Track learning ending time\n",
    "    exection_time = (end_time - start_time).total_seconds()  # Track execution time\n",
    "\n",
    "#   # Step 4: Results presentation\n",
    "    print(\"Learn: execution time={t:.3f} seconds\".format(t = exection_time))\n",
    "\n",
    "    print(\"R2: {:.2f}\".format(baseline.score(test_data,test_labels)))  # R2 should be maximize\n",
    "    mse = mean_squared_error(test_labels, y_pred)\n",
    "    print(\"MSE: {:.2f}\".format(mse))\n",
    "    print(\"RMSE: {:.2f}\".format(np.sqrt(mse)))\n",
    "    print(\"MAE: {:.2f}\".format(mean_absolute_error(test_labels,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
